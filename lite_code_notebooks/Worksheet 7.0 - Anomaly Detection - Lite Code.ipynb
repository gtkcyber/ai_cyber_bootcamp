{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "\n",
    "# Worksheet 7: Anomaly Detection - Lite Code\n",
    "\n",
    "This worksheet covers concepts relating to Anomaly Detection.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (https://matplotlib.org/stable/)\n",
    "* StatsModels (https://www.statsmodels.org/stable/index.html)\n",
    "* Pmdarima (https://alkaline-ml.com/pmdarima/)\n",
    "* Prophet (https://github.com/facebook/prophet)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pmdarima.arima import auto_arima\n",
    "from pmdarima.arima import ADFTest\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "DATA_HOME = '../data'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One:  Finding Anomalies in CPU Usage Data\n",
    "The first part of this lab, you will be examining CPU usage data to find anomalies. \n",
    "\n",
    "## Step One:  Get the Data\n",
    "For this example, we will be looking at CPU Utilization Data to see if we can identify periods of unusual activity.  The data can be found in several files:\n",
    "\n",
    "* `cpu-full-a.csv`:  A full set of CPU usage data without anomalies\n",
    "* `cpu-train-a.csv`:  The training set from data set A\n",
    "* `cpu-test-b.csv`:  The test set from data set A\n",
    "* `cpu-full-b.csv`:  A full set of CPU usage data with an anomaly\n",
    "* `cpu-train-b.csv`:  The training set from data set A\n",
    "* `cpu-test-b.csv`:  The test set from data set A\n",
    "\n",
    "\n",
    "This dataset is from examples in *Machine Learning & Security*  by Clarence Chio and David Freeman.  https://github.com/oreilly-mlsec/book-resources/tree/master/chapter3/datasets/cpu-utilization.\n",
    "\n",
    "First let's take a look at the data set A.  For the first part of this lab, load the training dataset into a dataframe.  DataFrames have an option `infer_datatime_format` which, when set to `True`, will automatically infer dates. Setting this will save time and steps in data preparation. \n",
    "\n",
    "Once the data is loaded, call the usual series of exploratory functions and most importantly, plot the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df = pd.read_csv(f'{DATA_HOME}/cpu-train-a.csv', parse_dates=[0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.sample(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Is the Data Stationary?\n",
    "\n",
    "Now, we are going to check to the stationarity of our data set.  Stationarity is a measurement of whether the data has seasonal trends or not.[1]\n",
    "\n",
    "First compute the rolling mean and standard deviation for the CPU column in the data set.  This can be accomplished with the `rolling` function.[2]  Try different window sizes. \n",
    "\n",
    "[1]: https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\n",
    "[2]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n",
    "\n",
    "Once you have computed the rolling mean and std, plot them on a graph with the original data.  If the lines are generally flat, we know that the data is stationary. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rolling_mean = df[<CPU COLUMN HERE>].rolling(15).mean()\n",
    "rolling_std = df[<CPU COLUMN HERE>].rolling(15).std()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(df[<CPU COLUMN HERE>], color=\"blue\",label=\"Original CPU Data\")\n",
    "plt.plot(rolling_mean, color=\"red\", label=\"Rolling Mean\")\n",
    "plt.plot(rolling_std, color=\"green\", label=\"Rolling Deviation\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to run a test called the Dickey-Fuller [1] test on this data to prove whether the data is stationary or not.  Use the `adfuller` method in statsmodels to perform this computation. (https://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.adfuller.html)  For this example, use `AIC` as the autolag parameter which means that the lag is chosen to minimize the information criterion.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Dickeyâ€“Fuller_test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your code here...\n",
    "adft = adfuller(df[<CPU COLUMN HERE>],autolag=\"AIC\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will present the results in a more understandable manner."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "print(output_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So is the Data Stationary? \n",
    "If the `p-value` is greater than 5 and the test statistics are greater than the critical values, then we know that the data is not stationary.  What do you think?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three:  Check for Autocorrelation\n",
    "The next step we want to determine is how correlated the time series is with past values. This will help us tune our model and also decide whether the data can be used at all.\n",
    "\n",
    "For this exercise, we will use the pandas `autocorrelation` methods.  \n",
    "\n",
    "#### References\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.autocorr.html\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the autocorrelation at various lag intervals. This is calculating the Pearson correlation, so 1 indiciates perfect correlation.\n",
    "\n",
    "At what point does the correlation go below 75%?  50%?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your code here... \n",
    "for i in range(1,20):\n",
    "    print(f\"{i} Period Lag: {df['cpu'].autocorr(lag=i)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an autocorrelation plot using the pandas autocorrelation plot method. This plot will help us visualize whether the data is correlated with itself and what the lag periods are.\n",
    "\n",
    "(https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html) \n",
    "\n",
    "The method is: `pd.plotting.autocorrelation_plot(<data>)`.\n",
    "\n",
    "The horizontal lines in the plot correspond to 95% and 99% confidence bands.  The dashed line is 99% confidence band."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "pd.plotting.autocorrelation_plot(df[<CPU COLUMN HERE>]).plot()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four:  Seasonal Decomposition\n",
    "The last analytic technique we're going to use here is seasonal decomposition. Using statsmodels `seasonal_decompose` create a decompose plot and let's take a look at the data.\n",
    "\n",
    "Use `additive` as the model type and try different values for the period. \n",
    "\n",
    "https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "decompose = seasonal_decompose(df[<CPU COLUMN HERE>],model='additive', period=<PERIOD VALUE>)\n",
    "decompose.plot()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automation\n",
    "\n",
    "While is good to understand how this works, the module `pmdarima` actually has an automated test that can do this automatically. Try running the code below to determine whether the data is stationary or not.\n",
    "\n",
    "```python\n",
    "adf_test = ADFTest(alpha=0.05)\n",
    "adf_test.should_diff(df['cpu'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "adf_test = ADFTest(alpha=0.05)\n",
    "adf_test.should_diff(df[<CPU COLUMN HERE>])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Five:  Fit an ARIMA Model\n",
    "Since we are dealing with time series data, let's train an ARIMA model and see how well this technique fits the actual data. \n",
    "\n",
    "ARIMA has three parameters:\n",
    "\n",
    "* `p`:  The number of lag observations included in the model\n",
    "* `d`: The number of times the raw observations are differenced\n",
    "* `q`:  The size of the moving average window\n",
    "\n",
    "We are going to use the auto_arima method in pmdarima to do our forecasting.  Let's see how it works.  First build and fit an ARIMA model setting seasonal to `True`.  \n",
    "\n",
    "\n",
    "Docs:\n",
    "https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "arima_model = auto_arima(df[<CPU COLUMN HERE>], seasonal=True, error_action='ignore')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the `summary()` method to view some summary statistics for this model.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Your code here...",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `predict_in_sample()` method, create a plot of the original data and the predictions to see how well the model did at forecasting with known data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['training_preds'] = arima_model.predict_in_sample()\n",
    "plt.plot(df['cpu'], color=\"blue\",label=\"Original CPU Data\")\n",
    "plt.plot(df['training_preds'], color=\"red\", label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three:  Find Anomalies in the CPU data\n",
    "Using data set `B` train a new model. Once you have a trained model, the next step is to call the `.predict()` method to generate 60 predictions.  \n",
    "\n",
    "Next, compare the predictions with the actual values in the test set, similar to how we assess the accuracy of a classifier.  We will call the difference between the actual and predicted value the anomaly score.  Calculate the anomaly score for the test data.  Finally, plot the anomaly scores, and see if you can find the time intervals with the highest anomaly score. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df2_train = pd.read_csv(f'{DATA_HOME}/cpu-train-b.csv', parse_dates=[0])\n",
    "df2_test = pd.read_csv(f'{DATA_HOME}/cpu-test-b.csv', parse_dates=[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df2_train.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df2_train[<CPU COLUMN HERE>].plot()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model2 = auto_arima(df2_train[<CPU COLUMN HERE>], seasonal=True, error_action='warn')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df2_test['predictions'] = model2.predict_in_sample(600)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df2_test['delta'] = df2_test[<CPU COLUMN HERE>] - df2_test[<PREDICTION COLUMN>]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df2_test[['cpu','predictions']].plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Another Approach: Prophet\n",
    "Another really useful library for time series analysis is called `Prophet` and is published by Meta.  The documentation is available here: https://github.com/facebook/prophet.  Prophet uses a unique algorithm for time series analysis, specifically it is based on a decomposable additive model where non-linear trends fit with seasonality.  Prophet can take seasons and holidays into account in its predictions.  Additionally, the algorithm seems to be fairly computationally efficient.\n",
    "\n",
    "The prophet equation is:\n",
    "``` python\n",
    "forecast = trend + seasonality + holidays + error term\n",
    "```\n",
    "\n",
    "## Using Prophet\n",
    "Prophet's usage is pretty straightforward, however it requires a dataframe with two columns: a timestamp and a data column.  These must be named `ds` and `y` respectively.  You may have to rename columns or create a view of your data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
