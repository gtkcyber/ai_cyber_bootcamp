{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "\n",
    "# Worksheet 6.0 Clustering\n",
    "\n",
    "This worksheet covers concepts relating to Unsupervised Learning.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (https://matplotlib.org/stable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:30:42.154611Z",
     "start_time": "2022-12-06T16:30:35.330346Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Real Data\n",
    "Let's try it on some real data and see what we can produce. As before the first step is to read in the data into a DataFrame.  \n",
    "\n",
    "We will be using this data later, but the dataset consists of approximately 6000 domains--5000 of which were generated by various botnets and 1000 are from the Alexa 1 Million.  The columns are:\n",
    "\n",
    "* `dsrc`:  The source of the domain\n",
    "* `domain`:  The actual domain\n",
    "* `length`:  The length of the domain\n",
    "* `dicts`:  Percentage containing dictionary words\n",
    "* `entropy`:  The entropy of the domain\n",
    "* `numbers`:  The number of digits in the domain\n",
    "* `ngram`:  Different n-grams which appear in the domain (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:31:31.132871Z",
     "start_time": "2022-12-06T16:31:31.048618Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/dga-full.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:31:34.152243Z",
     "start_time": "2022-12-06T16:31:34.141341Z"
    }
   },
   "outputs": [],
   "source": [
    "data['dsrc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Since clustering relies on measuring distances between objects it is important that all data points be on the same scale.  There are various methods for doing this, which are beyond the scope of this class, however, for this example, we will use scikit-learn's `StandardScaler` to accomplish this.  (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "The StandardScaler transforms each column by:\n",
    "* Subtracting from the element in each row the mean for each feature (column) and then taking this value and\n",
    "* Dividing by that feature's (column's) standard deviation.\n",
    "\n",
    "Scikit-learn has a transformer interface which is very similar to the other scikit-learn interfaces.  The basic steps are:\n",
    "1.  Create the Scaler object\n",
    "2.  Using the feature matrix, call the `.fit()` method to \"train\" the Scaler\n",
    "3.  Use the `.transform()` method to scale the data.\n",
    "\n",
    "**NOTE**: When using a Scaler, it is important to train the scaler on your data, and use this trained scalers on any future predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:32:46.682295Z",
     "start_time": "2022-12-06T16:32:46.676826Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = ['length', 'dicts','entropy','numbers','ngram']\n",
    "scaled_feature_columns = ['scaled_length', 'scaled_dicts','scaled_entropy','scaled_numbers','scaled_ngram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:33:02.844271Z",
     "start_time": "2022-12-06T16:33:02.828263Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Step 1:  Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Steps 2 & 3:  Fit the scaler and transform this data\n",
    "scaled_df = scaler.fit_transform(data[feature_columns])\n",
    "    \n",
    "#Put the scaled data into a dataframe\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=scaled_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data and you'll see that the data is now all scaled consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:33:42.423945Z",
     "start_time": "2022-12-06T16:33:42.409211Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for convenience, we're going to merge the scaled data with the non-scaled data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:33:47.397679Z",
     "start_time": "2022-12-06T16:33:47.389989Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "final_data = pd.merge( data, scaled_df, left_index=True, right_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "Now that we have data that is suitable (maybe) for clustering, in the section below, perform K-Means clustering on this data set.  Initially, start out with 2 clusters and assign the `cluster id` as a column in your DataFrame.\n",
    "\n",
    "Then do a `value_counts()` on the `dsrc` column for each cluster to see how the model divided the data.  Try various values for `k` to see how it performed.\n",
    "\n",
    "Remember to use the **scaled features** for your clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:34:18.450543Z",
     "start_time": "2022-12-06T16:34:18.284648Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=2)\n",
    "\n",
    "# fit the model to the data (scaled data only)\n",
    "\n",
    "kmeans.fit(# your code)\n",
    "\n",
    "# Get the values for our cluster centroids out of the resulting model\n",
    "centroids = kmeans.cluster_centers_\n",
    "# get the labels for each cluster ( cluster number/label that each point was assigned to)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "#print these to see what that data looks like\n",
    "print(centroids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put these labels and values into a dataframe so we can easily look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an idea of our data\n",
    "final_data['dsrc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['cluster_id'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "We can plot the results of our clusters, but only a few features at a time b/c we need each feature to be on an axis. So we can select 2 features at a time to plot and see what the results are and if it looks like the clusters are separated in a clear way. Can I draw a line between them? Is that a requirement?\n",
    "\n",
    "Try this plot using different scaled features. \n",
    "\n",
    "Try to make prettier colors than just 'Red' and 'Blue' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(10,2), columns=[\"A\", \"B\"])\n",
    "df = final_data.loc[:,['scaled_length','scaled_ngram', 'cluster_id']]\n",
    "dic = {0:\"Blue\", 1:\"Red\"}\n",
    "sns.scatterplot(x=\"scaled_length\", y=\"scaled_ngram\", data=df, hue=\"cluster_id\", palette = dic);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dig in\n",
    "We can use our dataframe to summarize these results and look at the details of these clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of each 'dsrc' found in cluster number 1\n",
    "final_data[final_data['cluster_id'] == 1]['dsrc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this DGA? let's grab some samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[final_data['cluster_id'] == 1]['domain'].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same process of printing out the 'dsrc' and 'domains' for cluster 0 and see if it looks like the model has clustered our data in to legit and benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Visualizing Performance\n",
    "As we already know, it is difficult to measure the performance of clustering models since there usually is no known ground truth from which to evaluate your model.  However, there are two techniques which \n",
    "\n",
    "The “elbow” method of selecting the optimal number of clusters for K-means clustering. K-means is a simple unsupervised machine learning algorithm that groups data into a specified number (k) of clusters. Because the user must specify in advance what k to choose, the algorithm is somewhat naive – it assigns all members to k clusters even if that is not the right k for the dataset.\n",
    "\n",
    "The elbow method: \n",
    "1. Calc k-means clustering on the dataset for a range of values for k (say from 1-10).\n",
    "2. For each value of k, compute Sum of squared distances of samples to their closest cluster center. Often called the Within-Cluster Sum of Squares.\n",
    "3. Plot these values for each k\n",
    "4. Determine best value of k based on inflection point (elbow) in the graph.\n",
    "   \n",
    "If the line chart looks like an arm, then the “elbow” (the point of inflection on the curve) is the best value of k. The “arm” can be either up or down, but if there is a strong inflection point, it is a good indication that the underlying model fits best at that point. \n",
    "\n",
    "\n",
    "### Your Turn!\n",
    "In the box below, create a visualization using the elbow method to see if there are any inflection points in the distortion score. That is, do you see an elbow in this graph? That should be the 'ideal' number of clusters for our k-means model of this data. \n",
    "\n",
    "Try different numbers for the sample and see how this affects the graph. \n",
    "Try differen values for the range of k to see how this changes the inflection point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:50:58.454328Z",
     "start_time": "2020-07-30T14:50:45.270908Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "WCSS=[]\n",
    "for i in range(1,3): # range of values of k to try\n",
    "  kmeans=KMeans(n_clusters=i,init='k-means++')\n",
    "  kmeans.fit(final_data[scaled_feature_columns].sample(50000))\n",
    "  WCSS.append(kmeans.inertia_)\n",
    "WCSS\n",
    "\n",
    "plt.plot(range(1,3),WCSS);\n",
    "plt.ylabel('Within-Cluster Sum of Squares')\n",
    "plt.xlabel('k=number of clusters')\n",
    "plt.title('Elbow method for determining k');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Way to Visualize Clustering Performance\n",
    "The Silhouette Coefficient is used when the ground-truth about the dataset is unknown and computes the density of clusters computed by the model. The score is computed by averaging the silhouette coefficient for each sample, computed as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, normalized by the maximum value. This produces a score between 1 and -1, where 1 is highly dense clusters and -1 is completely incorrect clustering. (http://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)\n",
    "\n",
    "\n",
    "### Your Turn!\n",
    "Using the YellowBrick `SilhouetteVisualizer`, try visualizing models with various values of `K`(n_clusters).\n",
    "\n",
    "**Note**:  This visualization is quite expensive, so I recommend performing this using a sample o your original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:51:20.839427Z",
     "start_time": "2020-07-30T14:51:17.730155Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer( KMeans(n_clusters=6, n_init=3))\n",
    "\n",
    "visualizer.fit(final_data[scaled_feature_columns].sample(10000))\n",
    "visualizer.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using Clustering methods\n",
    "Let's use a hierarchical clustering method to detect anomalies in a set of data points. The one we will use is called [agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). \n",
    "\n",
    "First we will create an anomaly dataset that only has 20 DGA rows and 1000 legit samples. Thus, there are 20 anomalies in this dataset that we know of. Then we will see if our clustering method can detect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the data frame with the domain name so we can spot check the model's performance\n",
    "\n",
    "dga_data = pd.read_csv('../data/dga_features_final_df_domain.csv')\n",
    "\n",
    "print(dga_data.isDGA.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dga = dga_data[dga_data['isDGA']==1].sample(20)\n",
    "not_dga = dga_data[dga_data['isDGA']==0]\n",
    "print(len(is_dga))\n",
    "print(len(not_dga))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data = pd.concat([is_dga, not_dga])\n",
    "dga_anomaly_data['isDGA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data_domain = dga_anomaly_data\n",
    "dga_anomaly_data = dga_anomaly_data.drop('domain',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use this dataset that conatins only a few anomalies in clustering\n",
    "\n",
    "First we want to scale the data because it is clustering which means that distance is important and that is extremely sensitive to different scales. The code is below, but if you want you can try to use  the Min Max Scaler after you fir the model to see if it changes the resuls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data  = StandardScaler().fit_transform(dga_anomaly_data.loc[:,'ngrams'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the data, we can put it into a model. Call the Agglomerative Clustering model from sklearn and fit it to the data. name the model **agglomerative_clust** and use the hyperparameters of \n",
    "\n",
    "```\n",
    "distance_threshold=0\n",
    "\n",
    "n_clusters=None\n",
    "```\n",
    "\n",
    "We are using these because we want to see all the data possible clusters in a dendrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clust = AgglomerativeClustering(#your hyperparameters here)\n",
    "\n",
    "\n",
    "agglomerative_clust = # Your code here\n",
    "\n",
    "# pull out the label of the cluster each point belongs to. \n",
    "labels_for_clusters = agglomerative_clust.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot_dendrogram function will plot the dendrogram for the cluster model you just fitted to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the plot_dendrogram function to plot the dendrogram. You an get lower and lower levels if you increase the value of **p**. Try plotting a few values of **p** to see the difference and notice what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(agglomerative_clust, truncate_mode=\"level\", p=5);\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the plot there are some digits on the x-axis with no parenthesis. These are nodes that did not have more than 1 data point. We want to take a look at these because they are 'far' enough away from the rest of the data to warrant their own node, and thus could be an anomaly. \n",
    "\n",
    "Below, use the original pandas data frame to print the rows of the single nodes and see if these are our few DGAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data_domain.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data_domain.iloc[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_anomaly_data_domain.iloc[29,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is just to look at the contents of the smallest cluster (depending on how small you think your anomalies are. IN this case our smallest cluster contains the DGAs. Thus, our model accurately pulled out the DGAs into their own 'anomaly' cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## DBSCAN\n",
    "Now that you've tried K-Means, let's try doing some clustering using DBSCAN (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  Remember that the main tuning parameters for DBSCAN are:\n",
    "\n",
    "* **epsilon (eps)**:  The minimum distance between two samples \n",
    "* **min_samples**:  The minimum number of samples needed to form a neighborhood\n",
    "\n",
    "By default epsilon is 0.5 and the min_samples is 5. \n",
    "\n",
    "1. First, try DBSCAN with the default options.  If you use the `fit_predict()` function, you can save the results in a new column in your data. The output for DBSCAN is similar to kmeans, so take a look at that code for hints. \n",
    "\n",
    "2. How did this compare with K-Means?  Given that you actually know what the data really is, how did DBSCAN do in terms of identifing meaningful clusters?\n",
    "\n",
    "3. Look at the `dsrc` column and do `value_counts()` for the various neighhborhoods.  What did you notice?\n",
    "\n",
    "4. Try again, but this time experiment with the values of epsilon and min_samples and see what DBSCAN comes up with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
